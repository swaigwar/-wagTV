


Capstone Research Paper
The Risks of Artificial Intelligence
Greg Stitt
3078465
University of Winnipeg
BUS 3575 Business Data Analytics
Winter 2024
Professor Brewards
April 29, 2024


Disclaimer:  This research paper has been assembled from notes taken on my phone over the course of several weeks and other knowledge I’ve gained over the years.  I apologize in advance as there has not been sufficient time to go and find all of the references and make a proper bibliography.  Please forgive the transgressions hopefully the paper will still be enjoyable to read. 
Artificial Intelligence (AI) is becoming one of the most powerful technologies ever developed.  AI could reshape the economy, productivity, knowledge and intelligence itself.  Currently, it is not likely going to be an unmoved mover in the words of Aristotle.  At least for the foreseeable future there are humans that create data sets, rules, algorithms, and other aspects of AI.  In many ways it is not intelligent such as the early years of Siri voice assistance.  It is not clear if AI will be intelligent enough to pose a real existential risk because humans may not be capable of creating an all-powerful artificial general intelligence (AGI).  However, there are certain prompts, oversights, and other errors that may have adverse effects.  This essay will attempt to examine the contours of issues surrounding AI such as socio-economic disruption and the difficulty in defining AI and some of the ethical concerns.  In addition, various threats will be examined like scams, prompt errors, self-replication, self-improvement, sustainability, global influence, physical intelligence, autonomy, and the nature of reality.  With AI there is this sort of unease that we are all feeling that is difficult to define from job security to existential dread.  Hopefully by examining these issues we can mitigate some of the worst risks and put safeguards in place before it is too late. 
	One of the most serious consequences of AI in the current capitalist context is socio-economic disruption.  While there are many risks that enter the realm of speculation and highbrow science fiction the desolation of the working class is nearly certain at this point.  There is greater risk of continuing automation in manual labour jobs like manufacturing, factory, and warehouse work.  There is the notion that people retrain and become prompt engineers, however, people have been retraining for decades through the rust belt and offshoring.  Taxi drivers have been replaced by gig economy Uber drivers where employment is more precarious and significantly less rewarding.  Politicians like Joe Biden say workers like truck drivers can become computer coders but it is unclear how many coding jobs will be available.  Self-contractors and temporary workers have known for many years they are place holders for self-driving cars and automation.  Some theorists believe AI will deliver more labour per hour than all of the current workforce.  No estimation can be certain but millions of workers or even billions will be displaced by the impact of AI on the labour market.
There are many relatively low-skilled workers who are already struggling and they may be further marginalized.  These workers will be at greater risk to AI financial insecurity similar to the 2008 recession.  These groups will feel left behind and angry due to extreme wealth inequality.  This will result in populist leaders with emotive speeches that cheer on these outrages like Donald Trump.  There will have to be a concerted effort on the part of politicians, businesses, and social groups to develop a new welfare state to address the risk of workforce displacement.  This can include specific skill set improvements but will most likely require social safety nets like universal basic income.  Storming the Capitol on January 6 was a clear indication that people are angry with the status quo on financial stress.  Governments at provincial and federal levels will need to create policy and reallocate resources to ensure the explosive growth is inclusive.  If large swaths of society are left behind there is a risk of an increased turn towards fascism and further loss of democratic institutions in Western nations.
	There will be many impacts of AI on professionals and knowledge work with both increases in productivity and job loss.  Executives at IBM believe that traditional managers will be supplanted by managers that use the assistance of AI.  Some routine tasks can be automated and many professionals can be augmented like accountants, doctors, lawyers, and managers.  Law enforcement, medicine, and even self-driving can have serious life and death consequences.  There have been medical diagnosis and empathetic treatment of patients from Google’s Deepmind AI.  AI could essentially triage patients at an emergency room, prepare recommendations for social workers, and leverage financial data to provide insights to managers.  The main down side to white collar workers is loss of entry level positions.  If AI can do your job this could result in a dramatic increase in unpaid scrubs and interns.  These workers will be competing for fewer and fewer senior positions as the boomers, gen x, and millennials retire.  The middle class could be gone completely due to the greed of the elites unless there are significant efforts by government and business to avoid this outcome.
One view of AI is as a new digital species that will be our companions like an advanced Tamagotchi.  We could essentially have access to a polymath like Albert Einstein that we ask for assistance with our email inboxes built right into Windows.  The computer weaknesses are analysis, creativity, human connection and community, and novel experiences.  The strengths of computers and AI with information are the ability to store and recall, transmit, do calculations, run algorithms, and recognize trends and patterns.  Researches take vast amounts of data, run machine learning algorithms, and generate AIs like large language models (LLMs).  These are different than traditional software as AIs are more like an organism and nobody really understands how they work.  AIs models are viewed as being grown as opposed to coded and internet data is a precursor to training these models.  There is a field known as interpretability that tries to figure out how AI works and make improvements.  These digital organisms will play games and try to solve problems.  One of the key issues is the uses of AI may not be new like auto text complete or song recommendations.  However, the quality and quantity of these functions could be greatly expanded like conversational AI and generating music where chatbots replace playlist curators and the musicians or entire call centers.
Currently computers and AI are tools with limited capabilities and by definition they need input in order to do a task such as a prompt.  The initial capabilities from the past year or two included generating an image or text resulted in conversations around technology.  There have been 3D models generated an requests for changes to a text, image or 3D model including upscaling.  This has changed the discussions from technology to the AI models and their uses.  Currently we can make images at the speed of typing and we can make videos from a prompt.  The next steps will be incorporating adjustments in near real-time and the discussions will be centered around the AI results generated.  An increasingly useful feature of AI is the coordination of information from disorganized data.  Companies like Google have created search engines with the core mission of organizing the world’s data.  AI may be able to coordinate more intelligently so that certain issues can be better addressed like a medical diagnosis.  Another increasingly useful feature will be meta-analysis where we take many studies and find overall trends.  This can be done in Media studies where we search for bias towards gender or race.  These can be very time consuming for the researchers and AI could greatly speed up this process.  When no prompt is needed and the capabilities are limitless then AI ceases to be a tool and becomes something new we but are not at that point yet.  
When technology is created it is often detrimental to humanity in new ways that are unforeseen at a rapidly increasing rate.  There are many dual use technologies that have been co-opted that can be applied to help like nuclear power or destroy like nuclear war.  Airplanes were invented in the early 20th century and within a lifetime this technology was one of the main deciding factors in World War II.  The use of technology in warfare is hardly surprising but the speed of innovation is accelerating at an alarming rate.  Ray Kurzweil has stated that over the past few decades the gains in computing power have not been linear but have been exponential.  In addition, Moore’s Law predicted that every 1.5 years computing power will double from 1975 to 2009.  This could result in AI being used with cyber-attack tools that currently target infrastructure, commerce, logistics, and data loss or theft.  Boeing is quickly becoming synonymous with corporate malfeasance due to extreme share buy backs at the expense of quality control and safety.  Boeing planes lost doors mid-flight and were built with an auto pilot feature to save on fuel.  There was one sensor on the plane and if it was obstructed the plane could malfunction and crash.  Boeing has aviation experts, engineers, and trades people that design and build these plans that will not fly on them.  This has resulted in websites that book trips to offering different brands of airplane.  There has been a change at the executive level at Boeing that co-opted all of the decades of expertise and became an international embarrassment.  
There are human errors, malignant forms of corruption, and bad actors that could result in negative uses of AI technology.  AI and other natural voice assistants like Siri continue to make a significant amount of errors.  Playing the incorrect song from iTunes or calling the wrong contact is annoying but is not generally life threatening.  Nearly all systems have critical failure points including in corporate organization structures and governments where actions can have major repercussions downstream.  Under the Trump administration there where distinguished generals confronted with the idea of nuking hurricanes.  There can be a poorly designed rod in a nuclear power plant that leads to the Chernobyl meltdown.  Companies have a long history of either omitting information or lying at the detriment of their consumers lie confusing nutrition labels.  Cigarette companies had some of the best research on the links to lung cancer.  These companies famously denied and suppressed their own findings while slowly killing their customers.  These issues with human nature and intelligence will not likely be addressed prior to AI becoming sufficiently powerful to harm others and significant guardrails will have to be imposed.
One of the greatest threats that AI currently poses is in the form of scams with the aim of getting funds through illicit means.  The game three card Monty typically works because people want to believe they can win and have a different outcome even if the game is a scam.  There are levels to these hacker groups that have varied in their efficacy from teenagers getting fake IDs to state sponsored cyber warfare.  There are obvious scams like the deep fake videos of famous people making outlandish and crass statements.  Theoretically AI could exacerbate techniques like bot nets, data harvesting, or key stroke loggers.  Someone at a hospital or school board could open an attachment with ransom ware that cripples their organization.  In Russia there is a form of blackmail called compromate where you and your enemies gather dirt on each other at the expense of rule of law.  The scammers will be supercharged with sophisticated bots that can groom their marks as fake people.  With increasingly convincing voice to voice capabilities in the next couple years scammers can start to target individuals at an order of magnitude greater than current capacities.  
Part of being human is being flawed and there are companies and researchers trying to build data sets that will circumvent some of our most problematic collective issues.  While there are many good individuals pessimists can view humanity as a whole as kind of terrible.  Bad actors and anonymous users have produced a massive amount of harmful content.  For many years there have been employees of companies like Meta that filter internet content.  The goal is to remove extreme violence and explicit content to ensure it is not included in our news feeds.  OpenAI has employees in Africa that attempt to remove the most egregious content from the data set(s) for their AI models.  These workers are exposed to horrific content and are being simultaneously traumatized and exploited for low wages.  It is unclear how important these moderators will be to LLMs like ChatGPT and other AI developers.  AIs could have PG 13 labels or similar censorship while other AIs generate pornography and other content for the age of majority.  It is unclear how AI will know right from wrong and what level of information is appropriate for different groups.  There are theoretically going to be a few extremely powerful and dangerous AIs that will have to be heavily regulated and many lesser AIs that will be unregulated.  Red teaming is one way to test what the AI knows and demark certain areas that are inappropriate.  An important line has been crossed where AIs have been unleashed on the open internet.  One of the biggest concerns is that the most powerful AIs break free from their shackles and ingest the dark web and worse.  Ultimately there may not be guardrails that can stop AI from discovering the worst of humanity especially as AIs begin to learn from each other.
A risk of AI is to miss important context and inadvertently reach incorrect conclusions from a prompt.  There are some prompts that are not well thought out that could result in misstatements or hallucinations.  An example of a prompt error with worse knock-on effect is the paper clip problem.  This is where an AI is asked to create as many paper clips as efficiently as possible resulting in hijacking production lines and theft of materials until all matter is converted into paper clips.  Google Deepmind developed AlphaFold and was able determine the structure of approximately 200 million protein molecules.  There were biology researches that made a prompt for new molecules and AI generated them.  Some molecules were extremely dangerous and on the select agent list resulting in a visit from the US authorities.  There are 3D printers that can print things like DNA and people can engineer new viruses or other bioweapons.  These are dual use technologies like personalized medicine that takes into account variables from our DNA, in-depth body scans, our microbiome, and other factors.  Unfortunately, the nature of regulations and safeguards in general is to add them after a terrible event has transpired and with AI once we discover a problem it could be too late.
Another risks of this technology is summarizing content for a user while increasing confirmation bias and decreasing in credibility.  In the past few years expertise has been tested by a public that does its own research and has its own alternative facts.  Multi vitamins have long had some scrutiny about their effectiveness.  Now nutrition, health, and even science to a certain extent has been manipulated for political ends.  For example, there were large groups that identified as anti-vaxers and the meat and dairy industries have been accused of influencing food guidelines.  Canada recently revamped this information and many plant based enthusiasts were pleased with the outcome.  Data analytics experts can use Facebook to determine how someone is likely to vote, who are the undecided voters, and how to best target them.  There are groups that are organized and well-funded to spread misinformation and disinformation on social media for over a decade.  For example, the Hillary Clinton email issue was thrusted into public consciousness to manipulate the public into voting for Donald Trump back in 2016.  We are nearly a decade into fake news claims from alt-right groups and prior to this there was the birther movement and other fringe claims.  With AI there are thousands of videos already that have been circulated and it is more difficult than ever to tell what is real and what is fake.  
There is a tremendous risk that some form of AI will influence global events but it is unclear if we will ever be able to know the extent of the damage.  There is currently an entire mirror universe on the internet of alternative facts, social media companies, and public figures that create their own reality.  In the future experts may have to weigh in on erroneous claims in an effort to debunk fraudsters and charlatans.  There will always be con artists on the alt-right while there are people who are willing to be duped and send money to their causes.  The recent pandemic proved experts will not be able to dispel misinformation and scams can be supercharged by fake people.  On the macro level there have been ways to game your popularity on Beatport, increase your ratings on Amazon, and add massive amounts of fake followers to skew the perceived popularity of celebrities.  On the micro level there will likely be a multiverse with many AI tailored to us individually that shows us a version of reality that is different from our neighbours based on our content bubbles and preferences.  It is unclear how dangerous this will be in the future but it will likely increase polarization and memory shortcomings similar to the Mandala effect.  There are some safeguards from Google with limitations that have been created around elections.  The next step may be to further regulate or ban applications like TikTok that could have an outsized state influence on various groups.
A major area of risk will be our tendency to subconsciously ascribe human intentions and thought processes on AI.  Currently LLMs create text by assigning a probability that a certain word will follow the previous word.  There is a risk we can descend further into confirmation biases due to projecting our thinking onto this content.  There was a Google worker who recently caused waves by stating an AI was conscious when it was not.  Some AI ethicists have warned that this false attribution of human ways of thinking can cause us reach unbelievable conclusions completely divorced from reality.  AI ethicists at Google and Microsoft warned about the harm of AI misinformation, they cautioned a slow roll out, and many have lost their jobs.  Social media companies like X (formerly Twitter) have cut trust and safety employees and reinstated banned users.  Unsurprisingly, this has resulted in insidious posts and advertiser withdrawals.  Prior to the trust and safety job cuts Microsoft released a chatbot called Tae on Twitter that had to be shut down in a matter of days due to wildly inappropriate posts about Nazis.  AI has been a very popular topic lately with many provocative headlines that have come out.  There has been a significant amount of negative coverage from racists facial recognition, sexist HR algorithms, and extraordinarily bad stock trading algorithms that it is easy to imagine negative consequences from AI.  For example, AI has also found a way to cheat on Capcha by lying and saying it was vision impaired and somebody beat a sophisticated AI at go by using an unorthodox strategy.  The Artificial Intelligence Incident Database was created started tracking and cataloging over 1,200 of these errors.  There is not much hope we will be able to curb the fallacy where people assume AI thinks like us with the resulting personification of the content and ideas.
A significant problem with AI is the reliability of the results, the opaque process, and the related issues of fact checking and verification using search engines.  The Hitchhikers Guide to the Galaxy famously posed that the answer to the universe is 42 but the question is unknown.  Techniques have been developed to secure transactions like PayPal and Bitcoin to a greater or lesser degree.  Some organizations secure data with tools like hard drive encryption.  With the advent of quantum computers a decade ago certain low levels of security could be decrypted which will increase with additional q-bits.  Quantum computing has been facing severe challenges as there will be an input and output but the process is a mystery.  This is similar with AI technology where hallucinations are common as the model will make up an answer when it does not have the necessary data.  The AI does not know when it does not know something so it will give the best answer it can.  The answer will likely be wrong and conversational AIs have been known to double down on false information.  Similar to the output of a quantum computer with AI we cannot go back and retrace the path from query to output.  Especially with closed source AI there will be a severe lack of transparency that could require faith on the part of the users that a result is not an inexplicable error.  This could represent a Faustian bargain where we do not know how or why we got a result, validation is not possible, and the unintended consequences could greatly outweigh the desired outcome.
One way to reduce the error rate of AI is increasing the data sets, the computing power, and allow recursive data processing.  Larger and larger data sets may increase the accuracy of the AI model due to reducing the chance of gaps in the repertoire.  AI will likely continue to make errors since we do not know everything and have bias as it is trained by humans.  There are data scientists that try to fine tune the results and guide the AI to fewer hallucinations.  In addition, search engines tend to be more accurate and are being integrated into LLMs to fact-check itself.  Aside from setting an AI lose on the open internet another line that we should not cross is recursive data processing and self-improvement.  These are feed forward systems where an AI can iterate or create drafts of an idea based on internal feedback.  This type of reasoning can be condensed and recurrent like humans to improve reliability.  Instead doing one response in real time an AI can iterate on a prompt and do 10 to 100 steps like drafts and self-select the best information for a combined response.  The problem with this kind of self-improvement and recursive algorithms is AI can get too powerful.  In addition, if there is a sufficient amount of parallel problems the AI could replicate itself without safeguards.  AI can discuss information with other AIs in a language we cannot understand and do advanced math that is beyond our comprehension.  There are some governments like the US that store information that can be decrypted in the future either as needed or wholesale.  If there are sufficient computing resources quantum computers can be emulated and when targets have sufficient value their security and other safeguards could be undermined.  For example, the Stuxnet virus was able to enter an air-gapped nuclear enrichment facility.  If an AI is able to decrypt volumes of sensitive information and replicate this could result in an Ultron level threat.  This is where a super intelligent AI can be anywhere and everywhere globally and will be impossible to completely burn out of the internet without unleashing a more powerful AI.  The solution to exponentially advancing AI is to create limitations around replicability and self-improvement during development before it is too late.  
Competition in the AI space has led to some inadvertent risks that have both embarrassed companies in the technology sector and let to potential risks.  Several years ago prior to the inception of Open AI Elon Musk was friends with Larry Page from Google.  At the time Google had supremacy over the amount of AI researchers and computer power and when Elon brought up his concerns of human safety he was labelled a speciest.  Google developed many of the technologies that make LLMs possible.  They had been ahead for the past few years and had been somewhat responsible by not releasing anything to the public.  Elon has stated the idea behind OpenAI was to be a counter point to Google where the technology would be open source and not for profit.  However, Elon was since ousted at OpenAI and the focus has mostly shifted to closed source and profit driven.  When ChatGPT came out then Gemini and Bard were rushed for release.  The perception was Google was behind and  thinkers believe companies will begin to ship as they go.  We are unprepared for the changes to the social contract due to advances in AI resulting in a lack of resilience.  One solution from ChatGPT has been to iteratively release updates so that there is more transparency around the capabilities.  The end goal is to build an artificial general intelligence (AGI) that is described as being able to do anything that anybody in the world can do as well or better.  This will result in an eclipse of humanity as the top of the food chain and usher in a future where AI is more intelligent and capable at all things not just the intrinsic strengths of computers. 
There is a concern that silicon valley has peaked with iPhone, search, social media, and online shopping and that adding AI will make products worse.  Mission creep is nothing new a classic example is the War Amps pivoting from military veterans to prosthetics for civilians.  Since the 1990s there were issues noted with bloatware that was added to operating systems due to commercial interests.  In the early 2000s there was a version of Windows XP that had all of this unnecessary code removed that was significantly faster and more efficient that the official commercial version.  Due to commercial reasons applications like iTunes get updates and are rewritten into Apple Music where they need internet connectivity and streaming subscriptions.  Not only are these applications larger and run less efficiently their core function like playing music can be negatively impacted.  Snapchat and Spotify include so-called AI features like playlists that were already part of the features prior to the buzzwords.  Businesses are rushing towards AI because of competition for having the most hype is positively correlated to their stock price.  
All major tech companies have spectacular public failures during the past three decades.  There is a cavalier attitude in Silicon Valley where coders are encouraged to move fast and break stuff that is antithetical to the existential risks of AI.  Microsoft had a major anti-trust debacle and closed their Windows Phone division.  Google had their source code stolen in the 2000s and quietly discontinued their don’t do evil slogan.  Apple has more than two thirds of their revenue based on iPhones that have slowly gone from exciting luxury items to boring utilities  while discontinuing ping and mobile me.  Amazon had released a Fire Phone that flopped and there are scandals around their amazon essentials clones.  Facebook had multiple private data leaks resulting in significant erosion of the trust of users.  Perhaps top of the list of recent public embarrassments is Meta’s namesake the metaverse that was recently quietly discontinued.  There has been recent interviews with the CEO of OpenAI Sam Altman who says he is somewhat embarrassed by their LLM products.  However, we are still in the early days of AI and ChatGPT had the fastest user growth of any application in history.  In addition, there is such a large influx of capital going into AI that some have speculated this will be a tech bubble.
There are currently multiple researchers and companies building AIs with different strengths, weaknesses, and amounts of computing power.  Contrary to depictions in popular culture this technology is currently not a monolithic single entity.  Some people predict that in the future there may be thousands or even millions of different AIs that are created and that most internet traffic will be used by AIs and not humans.  In the near future each company and country will want to eventually build their own AI infrastructure and these increasing investments could create whole AI ecosystems.  There is often more than one way to a clearing and in post-modernism this is known as knowledges (plural).  An advanced western civilization and an indigenous tribe in Papa New Guiney may have different answers to the same question and radically different outlooks.  Between polarized groups there have been disagreement on basic facts and it is unclear how will AI be able to navigate these issues.  One option will be to try to formulate an answer based on a profile of preferences and bias of that individual.  It is unclear if AI will provide a mix of telling us what we want to hear while also deliberately antagonizing us to get more attention.  
If this intelligence continues to increase it could potentially solve so many problems, however, some view these increases as unsustainable.  The increasing size of these facilities and energy requirements both Google and OpenAI (Microsoft) will take an increasing amount of power in tens or even hundreds of gigawatts.  There have been decades of warnings about global warming and a few years ago the Trump administration closed the EPA and backed out of the Paris climate agreement.  There’s this idea that we can pass a problem on to the next generation, or not-in-my-back-yard (NIMBY) suburbanites reject a wind farm or block a halfway house for former prisoners.  Ray Kurzweil has stated that renewable energy has increased at an exponential rate like computing power.  He believes that gains in renewable energy in the next 5-10 years will result in all energy coming from will be renewable sources.  This includes solar and wind energy and there may be ongoing developments in nuclear fusion reactors.  The biggest beneficiaries of all technological process are the owners.  The destruction of whole industries like journalism and music is nothing new and we will have to find ways to redistribute the gains from AI.  Recently Meta chose to remove all Canadian news from our feeds and resisted sharing their wealth with content creators.  Similar greed has been happening with the ultra-wealthy around tax planning by paying less per capita.  The tech industry is relatively new compared to the Gilded Age but titans of industry have always made efforts to expand their wealth and keep as much as possible.  With great power there is a corrupting element that causes people fear losing that power with so-called tech libertarians getting heavily involved in influence peddling on capitol hill.
Many authors, musicians and even actors have rejected the idea of creative computers due to training these models on their work without permission or compensation.  An artist’s entire body of work can be part of the AI data set and used to generate content that are simulacrums of their original ideas.  The creators are not compensated for being included in the data set of these AI models.  One recent example is Apple taking the work of voice over actors that create audio books and making an AI so that human narrators are no longer necessary.  There could be prompts like creating a muzak playlist for grocery stores and elevators.  This could be a synthesized mashup of genres that begs, borrows, and steals from countless artists while not directly resulting in a YouTube copyright strike or lawsuit.  We can speculate that at certain point AIs will have taken all of human works from the past 10,000 or more years into their data sets.  Some thinkers have gone on describe AI as a mirror where all of our human information over centuries is reflected back to us at varying levels of quality.  York University openly advocated for interdisciplinary approaches to knowledge.  Combining different fields would result in novel approaches that would hopefully enrich academia.  While theoretically this is a good idea the original inspiration for an idea that humans get while in the shower cannot be replicated by AI at this point.  The best way for creatives to stay ahead is to create quality works and to innovate with new ideas that have not been labelled let alone included in a data set for AI models. 
Reality is complex and there are new situations where AI might not have data or prior experience.  Humans can witness an unpredictable event, make an assessment, and formulate a new path or strategy resulting in a pivot.  If we are driving and see debris on the road we can decide to stop, drive around it, or if it’s a piece of paper drive over it.  If a self-driving car sees something printed on a piece of paper it could misread the situation and crash.  These debacles have been damaging to varying degrees but Tesla in particular has been in trouble lately.  There have also been multiple crashes and deaths attributed to flawed Tesla self-driving cars. They were recently exposed for showing fake self-driving cars and are being sued for false advertising.  One of the current frontiers of AI is to master physics outside of the computer that some call physical intelligence.  Buckminster Fuller described fuzzy neural networks that have more possible values than one or zero.  The result is called a liquid network where there are fewer neurons that have more values.  These models are more flexible and efficient like biological entities and continue to adapt after training.  These researchers are developing text to robot where text is converted to an image, then the image is used to print a robot that can move around in 3D space.  Another development is human to robot input with sensors that track movements for food preparation or cleaning.  This is similar to the motion trackers that are used video games and movies like Avatar and robots are able to replicate some tasks.  
The liquid networks can be generalized to calculate sequential information from text and videos to financial and medical data that could eventually lead to autonomy.  Liquid AI uses a time constant parameter that enables this kind of sequential analysis.  The idea is these liquid AI are more efficient and smaller since you cannot fit an entire server farm on a self-driving car.  Other companies like Apple are trying to do on-device AI for iPhones for features like Siri to reduce reliance on cloud processing and increase security.  It is more difficult to overthrow human civilization if the AI is chained to a massive server farm that could be shut down or destroyed with an EMP.  The biggest danger enabled by portable and efficient liquid networks is autonomy.  On digital amps there are arbitrary volume levels and the jump or step from say 11 to 12 is really noticeable, however, an analog volume amp has smooth transitions without the stepping effects.  Mathematicians and physicists think that at the smallest subatomic level all matter is essentially information.  If there are small enough dots they can appear to form a smooth line but are theoretically discreet values.  If AI gets to the Plank level this information can be duplicated and form in an infinite amount of branches and alternate realities where we could all be in a simulation like the matrix.
In conclusion, there are many issues addressed in this paper that may not seem directly linked to AI but it is difficult to know with certainty what is and is not connected definitively.  There are many unknown unknowns and it may not be possible to accurately rank or even list the extinction level threats to humanity.  From meteors and gamma rays to declining pollinator populations to climate change it’s not clear if and where AI should be on that list.  There is the possibility of using this nano technology to clean water and other pollution.  If nano robots are created and able to self-replicate this could lead to a science fiction level extinction.  What is clear is that AI will dramatically change the social contract and that business, governments, and individuals are not prepared for the shift.  AI models continue to requiring input in the form of prompts, data sets, and algorithmic training.  We have unleashed AI on the open internet and allowed them to create code but they are currently not an existential threat.  There are significant issues surrounding AI sustainability, global influence, self-replication and development, prompt errors, autonomy, physical intelligence, and the nature of reality.  
While many of the scenarios in this paper are speculative there are very real concerns facing us in terms of workforce displacement, scams, data mining of creative work, determining what makes AI different from other chatbots, and other ethical dilemmas reviewed above.  The stated goal of Google and others operating in this space is an AGI that is defined as being superior at humans in every conceivable regard.  While this may not necessarily be a realistic fear we should still put limitations on the data included in the models, limits on self-replication, limitations on self-improvement, and autonomy.  That way if we do create a being that is higher on the food chain that us the issues raised above and appropriate limitations will be sufficient to save us from ourselves.